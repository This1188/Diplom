import re
import string

class TextProcessor:
    """
    Упрощенный обработчик текстов без NLTK
    """
    
    def __init__(self, language='russian'):
        self.language = language
        
        # Базовые стоп-слова для русского языка
        self.stop_words = {
            'и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она',
            'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее',
            'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда',
            'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до',
            'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей',
            'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем',
            'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет',
            'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним',
            'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда',
            'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть',
            'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая',
            'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед',
            'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно',
            'всю', 'между'
        }
    
    def clean_text(self, text):
        """
        Очистка текста от лишних символов и приведение к нижнему регистру.
        """
        # Удаление специальных символов и цифр, оставляем только буквы и пробелы
        text = re.sub(r'[^а-яёa-z\s]', ' ', text.lower())
        # Удаление лишних пробелов
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def tokenize_text(self, text):
        """
        Простая токенизация текста на слова (разделение по пробелам).
        """
        return text.split()
    
    def remove_stopwords(self, tokens):
        """
        Удаление стоп-слов из списка токенов.
        """
        return [token for token in tokens if token not in self.stop_words and len(token) > 2]
    
    def process_text(self, text):
        """
        Полный процесс обработки текста.
        Возвращает очищенные и нормализованные токены.
        """
        cleaned_text = self.clean_text(text)
        tokens = self.tokenize_text(cleaned_text)
        filtered_tokens = self.remove_stopwords(tokens)
        return filtered_tokens
    
    def process_documents(self, documents):
        """
        Обработка списка документов.
        """
        processed_docs = []
        for doc in documents:
            processed_tokens = self.process_text(doc)
            processed_docs.append(processed_tokens)
        
        return processed_docs