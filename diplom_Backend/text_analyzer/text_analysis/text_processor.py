import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import re
from collections import Counter
import joblib
import os

class EnhancedTextProcessor:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–µ–∫—Å—Ç–∞ —Å —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ —Å–ª–æ–≤–∞—Ä—è–º–∏
    """
    
    def __init__(self):
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        self.stop_words = self._load_stop_words()
        
        # –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–ª–æ–≤–∞—Ä–∏
        self.theme_keywords = {
            '—Å–ø–æ—Ä—Ç': {
                '—Ö–æ–∫–∫–µ–π', '—Ñ—É—Ç–±–æ–ª', '–±–∞—Å–∫–µ—Ç–±–æ–ª', '—Ç–µ–Ω–Ω–∏—Å', '–≤–æ–ª–µ–π–±–æ–ª', '–º–∞—Ç—á', '–≥–æ–ª',
                '–∫–æ–º–∞–Ω–¥–∞', '–∏–≥—Ä–æ–∫', '—Å—á–µ—Ç', '–ø–æ–±–µ–¥–∞', '—Ç—É—Ä–Ω–∏—Ä', '—á–µ–º–ø–∏–æ–Ω–∞—Ç', '–æ–ª–∏–º–ø–∏–∞–¥–∞',
                '—Å–ø–æ—Ä—Ç—Å–º–µ–Ω', '—Ç—Ä–µ–Ω–µ—Ä', '—Å—Ç–∞–¥–∏–æ–Ω', '–ª–∏–≥–∞', '–ø–µ—Ä–≤–µ–Ω—Å—Ç–≤–æ', '—Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–µ',
                '—Ä–µ–∑—É–ª—å—Ç–∞—Ç', '—Ç–∞–∫—Ç–∏–∫–∞', '—Å—Ç—Ä–∞—Ç–µ–≥–∏—è', '–Ω–∞–ø–∞–¥–∞—é—â–∏–π', '–∑–∞—â–∏—Ç–Ω–∏–∫', '–≤—Ä–∞—Ç–∞—Ä—å'
            },
            '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏': {
                '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è', '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π', '–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç', '–ø—Ä–æ–≥—Ä–∞–º–º–∞', '–∞–ª–≥–æ—Ä–∏—Ç–º',
                '–∫–æ–º–ø—å—é—Ç–µ—Ä', '—Å–º–∞—Ä—Ç—Ñ–æ–Ω', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç', '–¥–∞–Ω–Ω—ã–µ', '–æ–±–ª–∞—á–Ω—ã–π',
                '—Ü–∏—Ñ—Ä–æ–≤–æ–π', '–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è', '—Ä–æ–±–æ—Ç', '—Å–µ—Ç—å', '—Å–µ—Ä–≤–µ—Ä', '–±–∞–∑–∞', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞',
                '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ', '–∏–Ω–Ω–æ–≤–∞—Ü–∏—è', '–≥–∞–¥–∂–µ—Ç', '—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ', '–æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è'
            },
            '—Ñ–∏–Ω–∞–Ω—Å—ã': {
                '—Ñ–∏–Ω–∞–Ω—Å', '—ç–∫–æ–Ω–æ–º–∏–∫', '—Ä—ã–Ω–æ–∫', '–∏–Ω–≤–µ—Å—Ç–∏—Ü', '–¥–µ–Ω—å–≥–∏', '–±–∞–Ω–∫', '–∫—Ä–µ–¥–∏—Ç',
                '–∞–∫—Ü–∏—è', '–±–∏—Ä–∂–∞', '–≤–∞–ª—é—Ç–∞', '–∏–Ω—Ñ–ª—è—Ü–∏—è', '–±—é–¥–∂–µ—Ç', '–∫–∞–ø–∏—Ç–∞–ª', '–ø—Ä–∏–±—ã–ª—å',
                '—É–±—ã—Ç–æ–∫', '–∫—É—Ä—Å', '–¥–∏–≤–∏–¥–µ–Ω–¥', '–æ–±–ª–∏–≥–∞—Ü–∏—è', '—Ç—Ä–µ–π–¥–µ—Ä', '–±—Ä–æ–∫–µ—Ä', '–∏–Ω–≤–µ—Å—Ç–æ—Ä',
                '–ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å', '–≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å', '–¥–µ—Ñ–æ–ª—Ç', '–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç'
            },
            '–ø–æ–ª–∏—Ç–∏–∫–∞': {
                '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç', '–º–∏–Ω–∏—Å—Ç—Ä', '–ø–∞—Ä–ª–∞–º–µ–Ω—Ç', '–≤—ã–±–æ—Ä—ã',
                '–∑–∞–∫–æ–Ω', '—Ä–µ—Ñ–æ—Ä–º–∞', '–¥–µ–º–æ–∫—Ä–∞—Ç–∏—è', '–¥–∏–ø–ª–æ–º–∞—Ç–∏—è', '–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–π',
                '—Å–∞–Ω–∫—Ü–∏—è', '–ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã', '–∫–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏—è', '–±—é—Ä–æ–∫—Ä–∞—Ç–∏—è', '–æ–ø–ø–æ–∑–∏—Ü–∏—è'
            },
            '–º–µ–¥–∏—Ü–∏–Ω–∞': {
                '–º–µ–¥–∏—Ü–∏–Ω', '–≤—Ä–∞—á', '–ø–∞—Ü–∏–µ–Ω—Ç', '–ª–µ—á–µ–Ω–∏–µ', '–¥–∏–∞–≥–Ω–æ–∑', '–±–æ–ª—å–Ω–∏—Ü–∞',
                '–∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–µ', '—Å–∏–º–ø—Ç–æ–º', '—Ç–µ—Ä–∞–ø–∏—è', '–æ–ø–µ—Ä–∞—Ü–∏—è', '—Ä–µ—Ü–µ–ø—Ç', '–≤–∏—Ä—É—Å',
                '–∏–º–º—É–Ω–∏—Ç–µ—Ç', '–≤–∞–∫—Ü–∏–Ω–∞', '—ç–ø–∏–¥–µ–º–∏—è', '–ø–∞–Ω–¥–µ–º–∏—è', '–∑–¥–æ—Ä–æ–≤—å–µ'
            }
        }
        
        # –°–∏–Ω–æ–Ω–∏–º—ã –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        self.synonyms = {
            '–∞–π—Ñ–æ–Ω': '—Å–º–∞—Ä—Ç—Ñ–æ–Ω',
            '–∏–∏': '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç',
            'ai': '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç',
            '–±–ª–æ–≥': '–±–ª–æ–≥–≥–µ—Ä',
            '–∫—Å–±': '–±–∞–Ω–∫',
            '–º–æ–±–∏–ª—å–Ω–∏–∫': '—Å–º–∞—Ä—Ç—Ñ–æ–Ω',
            '–Ω–æ—É—Ç': '–Ω–æ—É—Ç–±—É–∫',
            '–ø–∫': '–∫–æ–º–ø—å—é—Ç–µ—Ä',
            '—Å–æ—Ü—Å–µ—Ç—å': '—Å–æ—Ü–∏–∞–ª—å–Ω–∞—è —Å–µ—Ç—å',
            '—Ñ–∏–Ω': '—Ñ–∏–Ω–∞–Ω—Å',
            '—ç–∫–æ–Ω': '—ç–∫–æ–Ω–æ–º–∏–∫',
            '–∏–Ω–≤–µ—Å—Ç': '–∏–Ω–≤–µ—Å—Ç–∏—Ü'
        }
    
    def _load_stop_words(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤"""
        base_stop_words = {
            '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ',
            '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ',
            '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç', '–æ', '–∏–∑',
            '–µ–º—É', '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞', '–¥–∞–∂–µ', '–Ω—É', '–≤–¥—Ä—É–≥', '–ª–∏', '–µ—Å–ª–∏', '—É–∂–µ', '–∏–ª–∏',
            '–Ω–∏', '–±—ã—Ç—å', '–±—ã–ª', '–Ω–µ–≥–æ', '–¥–æ', '–≤–∞—Å', '–Ω–∏–±—É–¥—å', '–æ–ø—è—Ç—å', '—É–∂', '–≤–∞–º',
            '–≤–µ–¥—å', '—Ç–∞–º', '–ø–æ—Ç–æ–º', '—Å–µ–±—è', '–Ω–∏—á–µ–≥–æ', '–µ–π', '–º–æ–∂–µ—Ç', '–æ–Ω–∏', '—Ç—É—Ç', '–≥–¥–µ',
            '–µ—Å—Ç—å', '–Ω–∞–¥–æ', '–Ω–µ–π', '–¥–ª—è', '–º—ã', '—Ç–µ–±—è', '–∏—Ö', '—á–µ–º', '–±—ã–ª–∞', '—Å–∞–º', '—á—Ç–æ–±',
            '–±–µ–∑', '–±—É–¥—Ç–æ', '—á–µ–≥–æ', '—Ä–∞–∑', '—Ç–æ–∂–µ', '—Å–µ–±–µ', '–ø–æ–¥', '–±—É–¥–µ—Ç', '–∂', '—Ç–æ–≥–¥–∞',
            '–∫—Ç–æ', '—ç—Ç–æ—Ç', '—Ç–æ–≥–æ', '–ø–æ—Ç–æ–º—É', '—ç—Ç–æ–≥–æ', '–∫–∞–∫–æ–π', '—Å–æ–≤—Å–µ–º', '–Ω–∏–º', '–∑–¥–µ—Å—å',
            '—ç—Ç–æ–º', '–æ–¥–∏–Ω', '–ø–æ—á—Ç–∏', '–º–æ–π', '—Ç–µ–º', '—á—Ç–æ–±—ã', '–Ω–µ–µ', '—Å–µ–π—á–∞—Å', '–±—ã–ª–∏', '–∫—É–¥–∞',
            '–∑–∞—á–µ–º', '–≤—Å–µ—Ö', '–Ω–∏–∫–æ–≥–¥–∞', '–º–æ–∂–Ω–æ', '–ø—Ä–∏', '–Ω–∞–∫–æ–Ω–µ—Ü', '–¥–≤–∞', '–æ–±', '–¥—Ä—É–≥–æ–π',
            '—Ö–æ—Ç—å', '–ø–æ—Å–ª–µ', '–Ω–∞–¥', '–±–æ–ª—å—à–µ', '—Ç–æ—Ç', '—á–µ—Ä–µ–∑', '—ç—Ç–∏', '–Ω–∞—Å', '–ø—Ä–æ', '–≤—Å–µ–≥–æ',
            '–Ω–∏—Ö', '–∫–∞–∫–∞—è', '–º–Ω–æ–≥–æ', '—Ä–∞–∑–≤–µ', '—Ç—Ä–∏', '—ç—Ç—É', '–º–æ—è', '–≤–ø—Ä–æ—á–µ–º', '—Ö–æ—Ä–æ—à–æ',
            '—Å–≤–æ—é', '—ç—Ç–æ–π', '–ø–µ—Ä–µ–¥', '–∏–Ω–æ–≥–¥–∞', '–ª—É—á—à–µ', '—á—É—Ç—å', '—Ç–æ–º', '–Ω–µ–ª—å–∑—è', '—Ç–∞–∫–æ–π',
            '–∏–º', '–±–æ–ª–µ–µ', '–≤—Å–µ–≥–¥–∞', '–∫–æ–Ω–µ—á–Ω–æ', '–≤—Å—é', '–º–µ–∂–¥—É'
        }
        return base_stop_words
    
    def normalize_text(self, text):
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å —É—á–µ—Ç–æ–º —Ç–µ–º–∞—Ç–∏–∫"""
        text = text.lower()
        
        # –ó–∞–º–µ–Ω–∞ —Å–∏–Ω–æ–Ω–∏–º–æ–≤
        for wrong, correct in self.synonyms.items():
            text = re.sub(rf'\b{wrong}\b', correct, text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤, –Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∞–∂–Ω—ã—Ö
        text = re.sub(r'[^\w\s\.\,\-\:\+\%\$\‚Ç¨\¬£]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def extract_key_terms(self, text, max_terms=20):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ —Å —É—á–µ—Ç–æ–º —Ç–µ–º–∞—Ç–∏–∫"""
        words = text.split()
        
        # –í–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ —Å–ª–æ–≤ –ø–æ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Å–ª–æ–≤–∞—Ä—è–º
        weighted_terms = []
        for word in words:
            if len(word) < 3 or word in self.stop_words:
                continue
            
            weight = 1.0
            
            # –ü–æ–≤—ã—à–∞–µ–º –≤–µ—Å —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
            for theme, keywords in self.theme_keywords.items():
                if any(keyword in word for keyword in keywords):
                    weight *= 2.0  # –£–¥–≤–∞–∏–≤–∞–µ–º –≤–µ—Å –¥–ª—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ª–æ–≤
            
            # –ü–æ–≤—ã—à–∞–µ–º –≤–µ—Å —Ä–µ–¥–∫–∏—Ö —Å–ª–æ–≤ (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –æ–Ω–∏ –±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã)
            if len(word) > 6:
                weight *= 1.5
            
            weighted_terms.append((word, weight))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–µ—Å—É
        weighted_terms.sort(key=lambda x: x[1], reverse=True)
        
        # –ë–µ—Ä–µ–º —Ç–æ–ø-N —Ç–µ—Ä–º–∏–Ω–æ–≤
        top_terms = [term for term, _ in weighted_terms[:max_terms]]
        
        return ' '.join(top_terms)
    
    def process_document(self, text):
        """–ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        normalized = self.normalize_text(text)
        key_terms = self.extract_key_terms(normalized)
        return key_terms
    
    def guess_theme(self, text):
        """–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–º—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        normalized = self.normalize_text(text)
        words = normalized.split()
        
        theme_scores = {}
        for theme, keywords in self.theme_keywords.items():
            score = sum(1 for word in words if any(keyword in word for keyword in keywords))
            theme_scores[theme] = score
        
        if theme_scores:
            main_theme = max(theme_scores.items(), key=lambda x: x[1])
            if main_theme[1] > 0:
                return main_theme[0]
        
        return '–¥—Ä—É–≥–æ–µ'


class HybridTopicAnalyzer:
    """
    –ì–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–µ–º —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏
    """
    
    def __init__(self, models_dir='models'):
        self.processor = EnhancedTextProcessor()
        self.models_dir = models_dir
        os.makedirs(models_dir, exist_ok=True)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
        self.lda_params = {
            'n_components': 5,
            'max_iter': 100,
            'learning_method': 'online',
            'random_state': 42
        }
        
        self.nmf_params = {
            'n_components': 5,
            'random_state': 42,
            'beta_loss': 'frobenius',
            'max_iter': 1000
        }
        
        self.vectorizer_params = {
            'max_features': 5000,
            'min_df': 2,
            'max_df': 0.95,
            'ngram_range': (1, 3),
            'stop_words': list(self.processor.stop_words)
        }
        
        self.models = {}
        
    def prepare_corpus(self, documents, use_cache=True):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ—Ä–ø—É—Å–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        cache_file = os.path.join(self.models_dir, 'corpus_cache.pkl')
        
        if use_cache and os.path.exists(cache_file):
            print("–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ—Ä–ø—É—Å–∞ –∏–∑ –∫—ç—à–∞...")
            return joblib.load(cache_file)
        
        print("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        processed_docs = [self.processor.process_document(doc) for doc in documents]
        
        # TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        vectorizer = TfidfVectorizer(**self.vectorizer_params)
        X = vectorizer.fit_transform(processed_docs)
        feature_names = vectorizer.get_feature_names_out()
        
        result = {
            'X': X,
            'feature_names': feature_names,
            'processed_docs': processed_docs,
            'vectorizer': vectorizer
        }
        
        if use_cache:
            joblib.dump(result, cache_file)
            print(f"–ö–æ—Ä–ø—É—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –∫—ç—à: {cache_file}")
        
        return result
    
    def train_lda(self, X, n_topics=None):
        """–û–±—É—á–µ–Ω–∏–µ LDA –º–æ–¥–µ–ª–∏"""
        if n_topics is None:
            n_topics = self.lda_params['n_components']
        
        lda = LatentDirichletAllocation(
            n_components=n_topics,
            max_iter=self.lda_params['max_iter'],
            learning_method=self.lda_params['learning_method'],
            random_state=self.lda_params['random_state'],
            verbose=1
        )
        
        lda.fit(X)
        return lda
    
    def train_nmf(self, X, n_topics=None):
        """–û–±—É—á–µ–Ω–∏–µ NMF –º–æ–¥–µ–ª–∏"""
        if n_topics is None:
            n_topics = self.nmf_params['n_components']
        
        nmf = NMF(
            n_components=n_topics,
            random_state=self.nmf_params['random_state'],
            beta_loss=self.nmf_params['beta_loss'],
            max_iter=self.nmf_params['max_iter'],
            verbose=1
        )
        
        nmf.fit(X)
        return nmf
    
    def find_optimal_topics(self, X, max_topics=15):
        """–ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–µ–º"""
        print("\n–ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–µ–º...")
        
        silhouette_scores = []
        topic_range = range(2, min(max_topics, X.shape[0] - 1, X.shape[1] // 20) + 1)
        
        for n in topic_range:
            try:
                # –û–±—É—á–∞–µ–º KMeans –¥–ª—è –æ—Ü–µ–Ω–∫–∏
                kmeans = KMeans(n_clusters=n, random_state=42, n_init=10)
                cluster_labels = kmeans.fit_predict(X.toarray())
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Å–∏–ª—É—ç—Ç–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç
                if len(set(cluster_labels)) > 1:
                    score = silhouette_score(X.toarray(), cluster_labels)
                else:
                    score = -1
                
                silhouette_scores.append(score)
                print(f"  n={n}: silhouette={score:.4f}")
                
            except Exception as e:
                silhouette_scores.append(-1)
                print(f"  n={n}: –æ—à–∏–±–∫–∞")
        
        # –ù–∞—Ö–æ–¥–∏–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º
        if silhouette_scores:
            best_idx = np.argmax(silhouette_scores)
            best_n = topic_range[best_idx]
            print(f"\n–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º: {best_n} (silhouette={silhouette_scores[best_idx]:.4f})")
            return best_n
        
        # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        default_n = min(8, max(3, X.shape[0] // 5))
        print(f"\n–ò—Å–ø–æ–ª—å–∑—É–µ–º —ç–≤—Ä–∏—Å—Ç–∏–∫—É: {default_n} —Ç–µ–º")
        return default_n
    
    def extract_topic_keywords(self, model, feature_names, n_keywords=15, model_type='lda'):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Ç–µ–º"""
        topic_keywords = []
        
        if model_type == 'lda':
            components = model.components_
        elif model_type == 'nmf':
            components = model.components_
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {model_type}")
        
        for topic_idx, topic in enumerate(components):
            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            top_indices = topic.argsort()[:-n_keywords-1:-1]
            keywords = [feature_names[i] for i in top_indices]
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–º—É –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            theme_guess = self._guess_topic_theme(keywords)
            
            topic_info = {
                'topic_id': topic_idx,
                'keywords': keywords,
                'theme_guess': theme_guess,
                'topic_name': self._generate_topic_name(keywords, theme_guess)
            }
            topic_keywords.append(topic_info)
        
        return topic_keywords
    
    def _guess_topic_theme(self, keywords):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–º–∞—Ç–∏–∫–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        theme_scores = {}
        
        for theme, theme_keywords in self.processor.theme_keywords.items():
            score = 0
            for keyword in keywords[:10]:  # –°–º–æ—Ç—Ä–∏–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø-10 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                if any(theme_kw in keyword for theme_kw in theme_keywords):
                    score += 1
            theme_scores[theme] = score
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ü–µ–Ω–∫—É –¥–ª—è "–¥—Ä—É–≥–æ–π" —Ç–µ–º—ã
        theme_scores['–¥—Ä—É–≥–æ–µ'] = max(0, 10 - max(theme_scores.values()))
        
        return max(theme_scores.items(), key=lambda x: x[1])[0]
    
    def _generate_topic_name(self, keywords, theme):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–Ω—è—Ç–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–µ–º—ã"""
        # –ë–µ—Ä–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —Å–ª–æ–≤–∞ (–Ω–µ —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç—ã–µ –∏ –Ω–µ —Å–ª–∏—à–∫–æ–º —Ä–µ–¥–∫–∏–µ)
        if len(keywords) >= 3:
            # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º —Å–ª–æ–≤–∞ —Å—Ä–µ–¥–Ω–µ–π –ø–æ–∑–∏—Ü–∏–∏ –≤ —Å–ø–∏—Å–∫–µ
            mid_idx = len(keywords) // 2
            name_words = [keywords[0], keywords[mid_idx]]
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–ª–æ–≤–æ, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            theme_words = list(self.processor.theme_keywords.get(theme, []))
            if theme_words:
                for theme_word in theme_words[:3]:
                    if theme_word not in ' '.join(name_words).lower():
                        name_words.append(theme_word)
                        break
            
            topic_name = f"{theme.capitalize()}: {', '.join(name_words[:3])}"
        else:
            topic_name = f"{theme.capitalize()}: {', '.join(keywords[:3])}"
        
        return topic_name
    
    def assign_documents_to_topics(self, model, X, model_type='lda'):
        """–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–∞–º"""
        if model_type == 'lda':
            topic_dist = model.transform(X)
        elif model_type == 'nmf':
            topic_dist = model.transform(X)
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {model_type}")
        
        assignments = []
        for doc_idx, dist in enumerate(topic_dist):
            dominant_topic = np.argmax(dist)
            confidence = dist[dominant_topic]
            
            # –ï—Å–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∏–∑–∫–∞—è, –∏—â–µ–º –≤—Ç–æ—Ä—É—é –≤–æ–∑–º–æ–∂–Ω—É—é —Ç–µ–º—É
            secondary_topic = -1
            if confidence < 0.3:
                sorted_topics = np.argsort(dist)[::-1]
                if len(sorted_topics) > 1:
                    secondary_topic = sorted_topics[1]
            
            assignments.append({
                'document_index': doc_idx,
                'dominant_topic': dominant_topic,
                'confidence': float(confidence),
                'secondary_topic': int(secondary_topic),
                'topic_distribution': dist.tolist()
            })
        
        return assignments
    
    def ensemble_analysis(self, documents, use_cache=True):
        """
        –ê–Ω—Å–∞–º–±–ª–µ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
        """
        print("=" * 60)
        print("–ì–ò–ë–†–ò–î–ù–´–ô –ê–ù–ê–õ–ò–ó –¢–ï–ú–ê–¢–ò–ö")
        print("=" * 60)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        corpus_data = self.prepare_corpus(documents, use_cache)
        X = corpus_data['X']
        feature_names = corpus_data['feature_names']
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–µ–º
        optimal_topics = self.find_optimal_topics(X)
        self.lda_params['n_components'] = optimal_topics
        self.nmf_params['n_components'] = optimal_topics
        
        # –û–±—É—á–µ–Ω–∏–µ LDA
        print("\n" + "=" * 30)
        print("–û–ë–£–ß–ï–ù–ò–ï LDA –ú–û–î–ï–õ–ò")
        print("=" * 30)
        lda_model = self.train_lda(X)
        
        # –û–±—É—á–µ–Ω–∏–µ NMF
        print("\n" + "=" * 30)
        print("–û–ë–£–ß–ï–ù–ò–ï NMF –ú–û–î–ï–õ–ò")
        print("=" * 30)
        nmf_model = self.train_nmf(X)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        lda_keywords = self.extract_topic_keywords(lda_model, feature_names, model_type='lda')
        nmf_keywords = self.extract_topic_keywords(nmf_model, feature_names, model_type='nmf')
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        lda_assignments = self.assign_documents_to_topics(lda_model, X, 'lda')
        nmf_assignments = self.assign_documents_to_topics(nmf_model, X, 'nmf')
        
        # –ö–æ–Ω—Å–µ–Ω—Å—É—Å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        consensus_assignments = self._create_consensus_assignments(
            lda_assignments, nmf_assignments, optimal_topics
        )
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–µ–º–∞–º
        lda_stats = self._calculate_topic_statistics(lda_assignments, lda_keywords)
        nmf_stats = self._calculate_topic_statistics(nmf_assignments, nmf_keywords)
        consensus_stats = self._calculate_topic_statistics(consensus_assignments, lda_keywords)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        self.models['lda'] = lda_model
        self.models['nmf'] = nmf_model
        self.models['vectorizer'] = corpus_data['vectorizer']
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results = {
            'lda': {
                'topic_statistics': lda_stats,
                'keywords': lda_keywords,
                'assignments': lda_assignments,
                'model': lda_model
            },
            'nmf': {
                'topic_statistics': nmf_stats,
                'keywords': nmf_keywords,
                'assignments': nmf_assignments,
                'model': nmf_model
            },
            'consensus': {
                'topic_statistics': consensus_stats,
                'keywords': lda_keywords,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º LDA –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–æ–Ω—Å–µ–Ω—Å—É—Å–∞
                'assignments': consensus_assignments
            },
            'metadata': {
                'total_documents': len(documents),
                'optimal_topics': optimal_topics,
                'vocabulary_size': len(feature_names),
                'processing_time': '—Ä–µ–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å'
            }
        }
        
        # –í—ã–≤–æ–¥ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        self._print_diagnostics(results, documents)
        
        return results
    
    def _create_consensus_assignments(self, lda_assignments, nmf_assignments, n_topics):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Å–µ–Ω—Å—É—Å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        consensus = []
        
        for lda_assignment, nmf_assignment in zip(lda_assignments, nmf_assignments):
            doc_idx = lda_assignment['document_index']
            
            # –ï—Å–ª–∏ –æ–±–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ —Å–æ–≥–ª–∞—Å–Ω—ã
            if (lda_assignment['dominant_topic'] == nmf_assignment['dominant_topic'] and
                lda_assignment['confidence'] > 0.3 and nmf_assignment['confidence'] > 0.3):
                dominant_topic = lda_assignment['dominant_topic']
                confidence = (lda_assignment['confidence'] + nmf_assignment['confidence']) / 2
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å –±–æ–ª—å—à–µ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
                if lda_assignment['confidence'] > nmf_assignment['confidence']:
                    dominant_topic = lda_assignment['dominant_topic']
                    confidence = lda_assignment['confidence']
                else:
                    dominant_topic = nmf_assignment['dominant_topic']
                    confidence = nmf_assignment['confidence']
            
            consensus.append({
                'document_index': doc_idx,
                'dominant_topic': dominant_topic,
                'confidence': confidence,
                'secondary_topic': -1,
                'topic_distribution': [0] * n_topics
            })
        
        return consensus
    
    def _calculate_topic_statistics(self, assignments, topic_keywords):
        """–†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —Ç–µ–º–∞–º"""
        n_topics = len(topic_keywords)
        topic_docs = [[] for _ in range(n_topics)]
        
        for assignment in assignments:
            topic_id = assignment['dominant_topic']
            if 0 <= topic_id < n_topics:
                topic_docs[topic_id].append(assignment)
        
        stats = []
        for topic_id in range(n_topics):
            docs = topic_docs[topic_id]
            topic_info = topic_keywords[topic_id]
            
            if docs:
                avg_confidence = np.mean([doc['confidence'] for doc in docs])
            else:
                avg_confidence = 0
            
            stats.append({
                'topic_id': topic_id,
                'topic_name': topic_info['topic_name'],
                'theme_guess': topic_info['theme_guess'],
                'keywords': topic_info['keywords'],
                'document_count': len(docs),
                'average_confidence': round(avg_confidence, 3),
                'document_indices': [doc['document_index'] for doc in docs]
            })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        stats.sort(key=lambda x: x['document_count'], reverse=True)
        
        return stats
    
    def _print_diagnostics(self, results, documents):
        """–í—ã–≤–æ–¥ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        print("\n" + "=" * 60)
        print("–î–ò–ê–ì–ù–û–°–¢–ò–ß–ï–°–ö–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø")
        print("=" * 60)
        
        metadata = results['metadata']
        consensus_stats = results['consensus']['topic_statistics']
        
        print(f"\nüìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"  ‚Ä¢ –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {metadata['total_documents']}")
        print(f"  ‚Ä¢ –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º: {metadata['optimal_topics']}")
        print(f"  ‚Ä¢ –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {metadata['vocabulary_size']}")
        
        print(f"\nüéØ –ö–û–ù–°–ï–ù–°–£–°–ù–´–ï –¢–ï–ú–´:")
        for i, topic in enumerate(consensus_stats[:10]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-10 —Ç–µ–º
            if topic['document_count'] > 0:
                print(f"\n  –¢–µ–º–∞ #{i+1}: {topic['topic_name']}")
                print(f"    ‚Ä¢ –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º–∞—è —Ç–µ–º–∞—Ç–∏–∫–∞: {topic['theme_guess']}")
                print(f"    ‚Ä¢ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {topic['document_count']}")
                print(f"    ‚Ä¢ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {topic['average_confidence']:.3f}")
                print(f"    ‚Ä¢ –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(topic['keywords'][:5])}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–µ–º–∞–º
        print(f"\nüìà –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –î–û–ö–£–ú–ï–ù–¢–û–í –ü–û –¢–ï–ú–ê–ú:")
        for topic in consensus_stats:
            if topic['document_count'] > 0:
                percentage = (topic['document_count'] / metadata['total_documents']) * 100
                print(f"  ‚Ä¢ {topic['topic_name']}: {topic['document_count']} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({percentage:.1f}%)")
    
    def save_models(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        for name, model in self.models.items():
            filename = os.path.join(self.models_dir, f'{name}_model.pkl')
            joblib.dump(model, filename)
            print(f"–ú–æ–¥–µ–ª—å {name} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {filename}")
    
    def load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        for name in ['lda', 'nmf', 'vectorizer']:
            filename = os.path.join(self.models_dir, f'{name}_model.pkl')
            if os.path.exists(filename):
                self.models[name] = joblib.load(filename)
                print(f"–ú–æ–¥–µ–ª—å {name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {filename}")


class TrainedTopicAnalyzer:
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ–º–∞—Ç–∏–∫–∞—Ö
    """
    
    def __init__(self):
        self.hybrid_analyzer = HybridTopicAnalyzer()
        self.theme_classifier = ThemeClassifier()
        
    def analyze_with_training(self, documents, train_new=False):
        """
        –ê–Ω–∞–ª–∏–∑ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        if train_new or not os.path.exists(os.path.join(self.hybrid_analyzer.models_dir, 'lda_model.pkl')):
            print("üîÑ –û–±—É—á–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
            results = self.hybrid_analyzer.ensemble_analysis(documents)
            self.hybrid_analyzer.save_models()
        else:
            print("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π...")
            self.hybrid_analyzer.load_models()
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            corpus_data = self.hybrid_analyzer.prepare_corpus(documents, use_cache=False)
            X = corpus_data['X']
            feature_names = corpus_data['feature_names']
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
            lda_model = self.hybrid_analyzer.models['lda']
            nmf_model = self.hybrid_analyzer.models['nmf']
            
            # –ê–Ω–∞–ª–∏–∑ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏
            lda_keywords = self.hybrid_analyzer.extract_topic_keywords(
                lda_model, feature_names, model_type='lda'
            )
            lda_assignments = self.hybrid_analyzer.assign_documents_to_topics(
                lda_model, X, 'lda'
            )
            lda_stats = self.hybrid_analyzer._calculate_topic_statistics(
                lda_assignments, lda_keywords
            )
            
            results = {
                'topic_statistics': lda_stats,
                'keywords': lda_keywords,
                'assignments': lda_assignments,
                'metadata': {
                    'total_documents': len(documents),
                    'model_type': 'pre-trained LDA'
                }
            }
        
        return results


class ThemeClassifier:
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª –∏ ML
    """
    
    def __init__(self):
        self.rules = self._build_classification_rules()
        
    def _build_classification_rules(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–º"""
        rules = {
            '—Å–ø–æ—Ä—Ç': [
                (['—Ö–æ–∫–∫–µ–π', '–º–∞—Ç—á', '–≥–æ–ª', '–∫–æ–º–∞–Ω–¥–∞'], 2.0),
                (['—Ñ—É—Ç–±–æ–ª', '–≥–æ–ª', '–ø–µ–Ω–∞–ª—å—Ç–∏', '–æ—Ñ—Å–∞–π–¥'], 2.0),
                (['–±–∞—Å–∫–µ—Ç–±–æ–ª', '—Ç—Ä–µ—Ö–æ—á–∫–æ–≤—ã–π', '–¥–∞–Ω–∫', '–ø–æ–¥–±–æ—Ä'], 2.0),
                (['—Ç–µ–Ω–Ω–∏—Å', '—ç–π—Å', '—Å–µ—Ç', '–≥–µ–π–º'], 2.0),
                (['–æ–ª–∏–º–ø–∏–π—Å–∫–∏–π', '–º–µ–¥–∞–ª—å', '—Ä–µ–∫–æ—Ä–¥', '—Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–µ'], 1.5)
            ],
            '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏': [
                (['–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç', '–Ω–µ–π—Ä–æ—Å–µ—Ç—å', '–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ'], 3.0),
                (['—Å–º–∞—Ä—Ç—Ñ–æ–Ω', '–≥–∞–¥–∂–µ—Ç', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ'], 2.0),
                (['–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ', '–∞–ª–≥–æ—Ä–∏—Ç–º', '–∫–æ–¥', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞'], 2.0),
                (['–∏–Ω—Ç–µ—Ä–Ω–µ—Ç', '—Å–µ—Ç—å', '–æ–Ω–ª–∞–π–Ω', '—Ü–∏—Ñ—Ä–æ–≤–æ–π'], 1.5)
            ],
            '—Ñ–∏–Ω–∞–Ω—Å—ã': [
                (['–∞–∫—Ü–∏—è', '–±–∏—Ä–∂–∞', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏—è', '—Ç—Ä–µ–π–¥–µ—Ä'], 2.5),
                (['–±–∞–Ω–∫', '–∫—Ä–µ–¥–∏—Ç', '–∏–ø–æ—Ç–µ–∫–∞', '–≤–∫–ª–∞–¥'], 2.0),
                (['–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞', '–±–∏—Ç–∫–æ–∏–Ω', '–±–ª–æ–∫—á–µ–π–Ω'], 2.5),
                (['—ç–∫–æ–Ω–æ–º–∏–∫–∞', '–∏–Ω—Ñ–ª—è—Ü–∏—è', '–≤–∞–ª—é—Ç–∞', '—Ä—ã–Ω–æ–∫'], 2.0)
            ]
        }
        return rules
    
    def classify_document(self, text, keywords=None):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ —Ç–µ–º–∞–º"""
        text_lower = text.lower()
        
        theme_scores = {}
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–∞–≤–∏–ª–∞
        for theme, theme_rules in self.rules.items():
            score = 0
            for keywords_list, weight in theme_rules:
                keyword_count = sum(1 for keyword in keywords_list if keyword in text_lower)
                score += keyword_count * weight
            
            # –£—á–∏—Ç—ã–≤–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ LDA/NMF –∞–Ω–∞–ª–∏–∑–∞
            if keywords:
                for keyword in keywords[:10]:
                    for theme_rules_keywords, _ in self.rules.get(theme, []):
                        if any(kw in keyword for kw in theme_rules_keywords):
                            score += 1
            
            theme_scores[theme] = score
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º scores
        total_score = sum(theme_scores.values())
        if total_score > 0:
            theme_scores = {k: v/total_score for k, v in theme_scores.items()}
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Ç–µ–º—É
        if theme_scores:
            main_theme = max(theme_scores.items(), key=lambda x: x[1])
            if main_theme[1] > 0.1:  # –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                return {
                    'main_theme': main_theme[0],
                    'confidence': main_theme[1],
                    'all_scores': theme_scores
                }
        
        return {
            'main_theme': '–¥—Ä—É–≥–æ–µ',
            'confidence': 1.0,
            'all_scores': {'–¥—Ä—É–≥–æ–µ': 1.0}
        }


# –§–∞–±—Ä–∏—á–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
def create_analyzer(mode='hybrid'):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞
    
    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        mode: 'hybrid' - –≥–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (LDA + NMF)
              'trained' - —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
              'simple' - –ø—Ä–æ—Å—Ç–æ–π LDA –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
    """
    if mode == 'hybrid':
        return HybridTopicAnalyzer()
    elif mode == 'trained':
        return TrainedTopicAnalyzer()
    elif mode == 'simple':
        from .bayesian_analyzer import EnhancedBayesianAnalyzer
        return EnhancedBayesianAnalyzer()
    else:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–µ–∂–∏–º: {mode}")